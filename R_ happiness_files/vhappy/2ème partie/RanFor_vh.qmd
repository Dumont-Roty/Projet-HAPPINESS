---
title: "Net_Don_Happiness"
format: html
embed-resources: true
---

# Packages

```{r}
#| label: importation des packages
set.seed(1)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(MASS) #Modélisation LDA/QDA
library(naniar)
library(recipes)
library(dplyr)
library(rsample)
library(rpart)
library(rpart.plot)
library(doParallel)
library(ggplot2)
theme_set(theme_minimal())
library(readr)
library(randomForest)
library(hardhat)
```

# Importation des données

```{r}
data <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
                  na = c("", "NA"))
data <- data %>% mutate(across(everything(), ~replace_na(.x, "NA")))
data <- data %>% mutate(across(where(is.character), as.factor))
data <- data %>% mutate(across(where(is.logical), as.factor))
```
# Echantillonage

```{r}
set.seed(1)

data_split <- data |> initial_split(prop = 3/4)

test_data <- data_split |> testing()
train_data <- data_split |> training()
```

# Random forest + bagging

```{r}
rf <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  parms = list(split = "gini"), # paramètres
  na.action = na.omit
)
```

```{r}
nvar <- ncol(data) - 1
bag <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  parms = list(split = "gini"), 
  mtry = nvar, 
  na.action = na.omit
)
#L'argument mtry permet de définir le nombre de variable aléatoire qui sont candidates pour les découpages.
#Ici on a définit nvar commme le nombre de variable totale moins la variable à prédire (soit ici 15)
```

## Etude et optimisation manuelle

### Observation des sorties des commandes

*oob.times* *err.rates* *votes*

```{r}
rf$oob.times %>% head()

head(rf$oob.times) # a peu prés 15% des individus ne sont pas choisis
```

```{r}
rf$err.rate %>% head()
rf$err.rate %>% tail()
```

err.rate : taux d'erreur 
On obtient l'erreur out-of-bag 
-> chaque ligne correspond à l'erreur pour une forêt du nombre d'arbre correspondant à la ligne.
On a également le taux d'erreur pour les modalités de la variable à prédire. Erreur de prédiction pour les individus du groupe No et du groupe Yes.

-> ainsi on remarque qu'on fait plus d'erreur en mal classant les individus Yes

```{r}
rf$votes %>% head()
rf$votes %>% tail()
summary(rf$votes)
```
votes : quelle est la proportion d'arbres qui prédit No ou Yes pour un individu (un individu par ligne).
Ex : l'individu 3 est prédit majoritairement No par les arbres.

```{r}
data.frame(nb_trees = 1:rf$ntree,
           random_forest = rf$err.rate[,1], # OOB
           bagging = bag$err.rate[,1]) %>% # on a construit un data frame qui contient le nombre d'arbre et les erreurs
  tidyr::pivot_longer(cols = c(random_forest, bagging),
                      names_to = "model", values_to = "OOB_error") %>% 
  # pivot_longer premet de dire qu'on renvoit les valeurs de randomforest et bagging dans une variable qui s'appelle OOBerror et leur nom dans une variable qui s'apelle model
  ggplot() + 
  aes(x = nb_trees, y = OOB_error, col = model) +
  geom_line() +
  ylim(c(0.2, 0.45)) +
  theme_minimal()
```

Les erreurs OBB pour les deux modèles sont assez proches. 
Dans notre cas, le random forest est plus intéressant car on a plus d'arbre que le bagging

```{r}
data.frame(nb_trees = 1:rf$ntree,
           random_forest.no = rf$err.rate[,2], # no
           random_forest.yes = 1 - rf$err.rate[,3], #yes
           bagging.no = bag$err.rate[,2],#no
           bagging.yes = 1 - bag$err.rate[,3]) %>% # yes
  tidyr::pivot_longer(cols = c(random_forest.no, bagging.no, random_forest.yes, bagging.yes),
                      names_to = "model", values_to = "error") %>% 
  ggplot() + 
  aes(x = nb_trees, y = error, col = model) +
  geom_line() +
  ylim(c(0, 0.45)) +
  theme_minimal()
```

Que sur le modèle random Forest : 
```{r}
data.frame(nb_trees = 1:rf$ntree,
           All = rf$err.rate[,1],
           No = rf$err.rate[,2],
           Yes = 1-rf$err.rate[,3]) %>% 
  tidyr::pivot_longer(cols = c(All, No, Yes),
                      names_to = "Type", values_to = "OOB_error") %>% 
  ggplot() + 
  aes(x = nb_trees, y = OOB_error, col = Type) +
  geom_line() +
  ylim(c(0, 1)) +
  theme_minimal()

table(data_train$vhappy)
```

4. Faire varier le nombre de variables choisies à chaque cission et visualiser les erreurs OOB.

```{r}
# plusieurs rf pour différentes valeurs de mtry
rf1 <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  mtry =1,
  parms = list(split = "gini"))
rf3 <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  mtry =3,
  parms = list(split = "gini"))
rf5 <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  mtry =5,
  parms = list(split = "gini"))
rf7 <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  mtry =7,
  parms = list(split = "gini"))
rf9 <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  mtry =9,
  parms = list(split = "gini"))
rf13 <- randomForest(vhappy ~ .,
  data = train_data, method = "class", 
  mtry =15,
  parms = list(split = "gini"))
```

```{r}
# représentation graphique des erreurs OOB 
data.frame(nb_trees = 1:rf$ntree,
           rf_1 = rf1$err.rate[,1],
           rf_3 = rf3$err.rate[,1],
           rf_5 = rf5$err.rate[,1],
           rf_7 = rf7$err.rate[,1],
           rf_9 = rf9$err.rate[,1],
           rf_13 = rf13$err.rate[,1]) %>% 
  tidyr::pivot_longer(cols = starts_with("rf"),
                      names_to = "model", values_to = "OOB_error") %>% 
  ggplot() + 
  aes(x = nb_trees, y = OOB_error, col = model) +
  geom_line() +
  ylim(c(0.2, 0.45)) +
  theme_minimal()

# ici on fait des arbres très profond on n'a pas définit maxnoeud
```

5. Faire varier la profondeur des arbres construits (avec maxnodes) dans le modèle de bagging et visualiser les erreurs OOB.

```{r}
### choix de profondeur en bagging
errprofbag <- NULL
prof <- c(2, 5, 10, 15, 20, 30, 40, 50, 100)
i <- 1
for (k in prof) {
  rfprobag <- randomForest(vhappy ~ .,
    data = train_data, method = "class", ntree = 500,
    parms = list(split = "gini"), maxnodes = k, mtry = nvar, na.action = na.omit
  )
  errprofbag[i] <- rfprobag$err.rate[200, 1]
  i <- i + 1
}
```

```{r}
data.frame(prof = prof, error = errprofbag) |> 
  ggplot() + aes(x = prof, y = error) +
  geom_line() + ylim(c(0.1, 0.35))
```

6. Faire la même chose pour les modèles de forêts aléatoires et visualiser les erreurs OOB.

```{r}
## choix de profondeur en rf
errprofrf <- NULL
i <- 1
for (k in prof) {
  rfpro <- randomForest(vhappy ~ .,
    data = train_data, method = "class", ntree = 200, mtry = 2,
    parms = list(split = "gini"), maxnodes = k, na.action = na.omit
  )
  errprofrf[i] <- rfpro$err.rate[200, 1]
  i <- i + 1
}
```

```{r}
data.frame(prof = prof, error_bag = errprofbag, error_rf = errprofrf) |> 
  ggplot() + aes(x = prof, y = error_rf) +
  geom_line() + ylim(c(0.1, 0.35))
```

```{r}
data.frame(prof = prof, error_bag = errprofbag, error_rf = errprofrf) |> 
  tidyr::pivot_longer(cols = starts_with("err"), names_to = "model", values_to = "error") |> 
  ggplot() + aes(x = prof, y = error, col = model) +
  geom_line()  + ylim(c(0.1, 0.35))
```


7. Récupérer la taille de forêt minimisant l’erreur OOB.

8. On s’intéresse maintenant aux variables les plus importantes dans la construction des arbres. Pour obtenir cela effectuer les commandes suivantes.

```{r}
# importance des variables
rf <- randomForest(vhappy ~ .,
   data = train_data, method = "class", ntree = 200, mtry = 2,
   parms = list(split = "gini"), na.action = na.omit,
   keep.forest = FALSE, 
   importance = TRUE) # on spécfice au moment de la construction de la forêt 

lobstr::obj_size(rf)
# tout FALSE : 45kB, avec importance : 46kB
# avec forêt : environ 500kB

rf <- randomForest(vhappy ~ .,
  data = train_data, method = "class", ntree = 200, mtry = 6,
  parms = list(split = "gini"), na.action = na.omit,
  keep.forest = TRUE, importance = TRUE
)

lobstr::obj_size(rf)
# il faut avoir précisé importance=TRUE lors de la construction de la forêt
importance(rf) %>% head()

# commande toute faite pour représenter :
# variable importance plot
varImpPlot(rf, main = "Random Forest", cex = 0.8)
```

## Choix optimal des paramètres

On va chercher à optimiser le nombre de variables à considérer à chaque cission.
La profondeur des arbres (maxnodes, nodesize) et la taille de la forêt sont aussi des paramètres à traiter.

### Création/définition du workflow avec la paramètre mtry à optimsier.

#### ne fonctionne pas
```{r}
# recette
rec <- recipe(vhappy ~ ., data = train_data) # déjà créé avant

# spécification du modèle : randomforest
random_forest_spec <- rand_forest(mtry = tune()) |> #on optimise mtry 
  set_engine("randomForest", importance = TRUE) |> # par défaut l'engin est ranger 
  set_mode("classification")

tune_rf_wf <- workflow() |> 
  add_model(random_forest_spec) |> 
  add_recipe(rec)
```
Recherche du paramètre mtry optimal parmi les entiers via une validation croisée*
```{r}
data_cv <- vfold_cv(data_train, repeats=5) # validation croisée
# et on fait 5 validations croisées 

mtry_grid <- data.frame(mtry = 15)
# on crée une grille de valeur 
# on test les valeurs allant de 1 à 13

rf_tune_res <- tune_grid(
  tune_rf_wf, # le workflow
  resamples = data_cv, # les échantillons de validation croisée
  grid = mtry_grid, # la grille des valeurs à tester
  metrics = metric_set(accuracy) # la métrique pour choisir la meilleur valeur
)
 # environ 1 minute sur fixe perso
autoplot(rf_tune_res)
```

```{r}
rf_tune_res |> show_best(metric = "accuracy")
```

#### Optimisation de plusieurs paramètres, grille de paramètres

Dans le modèle de forêt aléatoire on peut optimiser la *profondeur des arbres* et *le nombre d'arbre* pour optimiser la mémoire utilisé

```{r}
args(rand_forest)

translate(random_forest_spec)

extract_parameter_set_dials(random_forest_spec)
```

```{r}
mtry()
```
#### Optimisation de tous les paramètres 

```{r}

random_forest_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = tune()) |> 
  set_engine("randomForest", importance = TRUE) |> 
  set_mode("classification")

tune_rf_wf <- workflow() |> 
  add_model(random_forest_spec) |> 
  add_recipe(rec)

extract_parameter_set_dials(tune_rf_wf) 
```

```{r}
expand_grid(mtry = 1:15, trees = c(100, 50, 200, 500)) # construction d'un tibble
expand.grid(mtry = 1:15, trees = c(100, 50, 200, 500)) # construction d'un datatab
crossing(mtry = 1:15, trees = c(100, 50, 200, 500))#valeurs ordonné
crossing(mtry = 1:15, trees = c(100, 50, 200, 500), min_n = c(2, 5, 10, 20, 50))
```

```{r}
# préciser ou changer les étendues
rf_param <- extract_parameter_set_dials(tune_rf_wf) |> #on extrait les paramètres
  update(mtry = mtry(c(1,15)), trees = trees(c(50,500))) 
grid_regular(rf_param, levels = 2) # on prend 2 levels pour les param
grid_regular(rf_param, levels = c(mtry = 15, trees = 4, min_n = 4))
# on précise le nombre de valeur qu'il prend, puis parmis la liste des valeurs proposé dans update il choisira de façon bien répartie 
```
```{r}
n_core = parallel::detectCores(logical = TRUE)
registerDoParallel(cores = n_core - 1) # on définit qu'on travaille sur plusieurs cores

# 13s avec 13 coeurs et mon ordi pro version 2024
# 5 minutes avec 8 coeurs sur mon ordinateur fixe pro
# 1 minutes avec 3 coeurs sur mon ordinateur fixe perso
rf_tune_res <- tune_grid(
  tune_rf_wf, 
  resamples = data_cv, 
  grid = grid_regular(rf_param, levels = c(mtry = 2, trees = 4, min_n = 4)), 
  metrics = metric_set(accuracy)
)

#
stopImplicitCluster()# ferme le cluster
```

```{r}
autoplot(rf_tune_res)  +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Avec RANGER

```{r}
ranger_rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = tune()) |> 
  set_engine("ranger", verbose = TRUE) |> 
  set_mode("classification")

tune_ranger_wf <- workflow() |> 
  add_model(ranger_rf_spec) |> 
  add_recipe(rec)
```

```{r}
registerDoParallel(cores = n_core - 1)
# 
ranger_tune_res <- tune_grid(
  tune_ranger_wf, 
  resamples = data_cv, 
  grid = crossing(mtry = c(1, 2, 3), trees = c(100, 50), min_n = c(2, 5, 10)), 
  metrics = metric_set(accuracy)
)
#
stopImplicitCluster()# ferme le cluster
```

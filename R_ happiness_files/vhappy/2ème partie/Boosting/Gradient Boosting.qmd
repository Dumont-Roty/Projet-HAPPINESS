---
title: "Gradient Boosting"
format: html
---

```{r}
# Bibliothèques nécessaires
library(tidymodels)
library(xgboost)       # Gradient Boosting
library(vip)           # Importance des variables
library(pROC)          # Analyse de la courbe ROC
library(doParallel)    # Parallélisation pour accélérer
library(future)
```

```{r}
data <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
                  na = c("", "NA"))
data <- data %>% mutate(across(everything(), ~replace_na(.x, "NA")))
data <- data %>% mutate(across(where(is.character), as.factor))
data <- data %>% mutate(across(where(is.logical), as.factor))
```

```{r}
set.seed(1)
data_split <- initial_split(data, prop = 0.75, strata = vhappy)
train_data <- training(data_split)
test_data <- testing(data_split)
```

```{r}
boost_recipe <- recipe(vhappy ~ ., data = train_data) |> 
  step_normalize(all_numeric_predictors()) |>  # Normalisation des variables numériques
  step_dummy(all_nominal_predictors()) |>     # Encodage des facteurs en indicateurs
  step_mutate(across(where(is.logical), as.numeric)) |> # Transformation des colonnes logiques en numériques
  step_zv(all_predictors()) |>                # Suppression des variables sans variance
  step_corr(all_numeric_predictors(), threshold = 0.9) # Suppression des variables très corrélées
```

```{r}
boost_spec <- boost_tree(
  trees = tune(),              # Nombre d'arbres
  learn_rate = tune(),         # Taux d'apprentissage
  tree_depth = tune()          # Profondeur des arbres
) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

```

```{r}
boost_wf <- workflow() |> 
  add_model(boost_spec) |> 
  add_recipe(boost_recipe)

```

```{r}
boost_grid <- grid_random(
  trees(range = c(500, 1500)),       # Nombre d'arbres
  learn_rate(range = c(0.01, 0.3)), # Taux d'apprentissage
  tree_depth(range = c(3, 10)),     # Profondeur des arbres
  size = 20                         # Taille de la grille
)
```

```{r}
# Validation croisée
data_cv <- vfold_cv(train_data, v = 5, strata = vhappy)

n_core <- parallel::detectCores(logical = TRUE)
plan(multisession, workers = parallel::detectCores() - 1)
```

```{r}
boost_tune_res <- tune_grid(
  boost_wf,
  resamples = data_cv,
  grid = boost_grid,
  metrics = metric_set(accuracy) # Évaluation avec la précision
)

# Arrêter la parallélisation après l'entraînement
plan(sequential)

```

```{r}
# Meilleurs paramètres
best_params <- boost_tune_res |> 
  select_best(metric = "accuracy")

# Finalisation du workflow avec les meilleurs paramètres
boost_final_wf <- boost_wf |> 
  finalize_workflow(best_params)


```

```{r}
boost_fit <- boost_final_wf |> 
  last_fit(data_split)

# Collecte des métriques
boost_metrics <- boost_fit |> collect_metrics()
print(boost_metrics)

boost_predictions <- boost_fit |> collect_predictions()
save(boost_predictions, file = "bp.RData") 

```

```{r}
confusion_matrix_boost <- boost_predictions |> 
  conf_mat(truth = vhappy, estimate = .pred_class)

autoplot(confusion_matrix_boost, type = "heatmap") +
  labs(title = "Matrice de Confusion - Boosting")

```


##  {.smaller}

```{r roc_boost}
summary(boost_predictions$.pred_yes)

```

la médiane est **très basse (0.15)**, alors que la moyenne est autour de **0.40**.

```{r}
# Sauvegarde du modèle final
save(boost_recipe,boost_spec,boost_wf,boost_grid,data_cv,boost_tune_res,best_params,boost_final_wf,boost_fit,boost_metrics,boost_predictions,confusion_matrix_boost, file = "boost_model_final.RData")  
```

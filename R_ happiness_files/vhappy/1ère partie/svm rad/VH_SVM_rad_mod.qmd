---
title: "LDA / QDA Very Happiness"
format: html
embed-resources: true
---

# Packages

```{r}
#| label: importation des packages
set.seed(123)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
```

# Importation des données

```{r}
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
                  na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
```

# 10% de la data
```{r}
#n <- nrow(ID_v6) #taille de la data
#N <- round(1/5*n) # Taille de l'échantillon d'entrainement
#train <- sample(1:n, size = N) # découpage de la data train
#ID_v7 <- ID_v6 %>% slice(train)
```

# Echantillonage 

```{r}
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
```

## Construction des différents modèles

### Définition des modèles :

```{r}
#| label: Définition des 5 modèles 

svm_rad_mod <- svm_rbf() |> 
  set_mode("classification") |> 
  set_engine("kernlab")
```

### recette à appliquer 

```{r}
# dat_rec <- dat_train |> recipe(vhappy~educ+kids+tvhours) #TROP LOOOOOOOOOOOONG

dat_rec <- dat_train  %>% recipe(vhappy~.) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_other(all_nominal_predictors(), threshold = 0.05) %>% 
  step_zv(all_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)
```

```{r}
svm_rad_wf <- workflow() |> 
  add_model(svm_rad_mod |> set_args(margin = tune(),rbf_sigma = tune())) |> 
  add_recipe(dat_rec)
```

### Echantillonage pour l'optimisation des paramètres

Pour l'optimisation des hyper paramètres --> ADOCK
```{r}
dat_folds <- vfold_cv(dat_train, v = 5, strata = vhappy) # 5 couches qui respectent tjrs le nombre gens très heureux
```

### Grille pour tester les hyper paramètres

svm radial

```{r}
# nombre de coeurs 
n_core <- parallel::detectCores(logical = TRUE)
registerDoParallel(cores = n_core -1) 
# on laisse un coeur pour le reste
# réalisation des commandes nécéssitant la parallélisation


svm_rad_grid <- svm_rad_wf |> extract_parameter_set_dials() |> grid_regular(levels = 5)
tune_res_svm_rad <- tune_grid(svm_rad_wf,
                              resamples = dat_folds,
                              grid = svm_rad_grid)

stopImplicitCluster()# ferme le cluster
```

```{r}
autoplot(tune_res_svm_rad)
```

```{r}
svm_rad_final_wf <- svm_rad_wf |> finalize_workflow((tune_res_svm_rad |> select_best(metric = "roc_auc")))
```

Les paramètres sont optimisés on peut maintenant faire travailler les modèles sur les données d'apprentissage et comparer les qualités des modèles sur les données test.

```{r}
collect <- function(x){
  last_fit(x,split = split_dat) |> # "je veux travailler sur tel truc en sachant le partage de donnée"
    collect_predictions()
}
```

```{r}
svm_rad_result <- svm_rad_final_wf |> collect()
```

On va construitre un tableau qui continent les résultats pour chaque individus de la base de donnée testing, en fonctioon de la méthode utilisé
Pour chasue individu : - résultat, -méthode utilisé



Comparons les modèles :

```{r}
svm_rad_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, svm_rad_result$.pred_no) %>% auc()
```
Si proche de la bissectrice, le hasard est aussi bon que le modèle.

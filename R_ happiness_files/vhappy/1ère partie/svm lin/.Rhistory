#| label: importation des packages
set.seed(123)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
n <- nrow(ID_v6) #taille de la data
N <- round(1/10*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
ID_v7 <- ID_v6 %>% slice(train)
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
#| label: Définition du modèle QDA
qda_mod <- discrim_quad() |>
set_mode("classification") |>
set_engine("MASS")
dat_rec <- dat_train  %>% recipe(vhappy~.) %>%
step_normalize(all_numeric_predictors()) %>%
# step_dummy(all_nominal_predictors()) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_zv(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = 0.9)
qda_wf <- workflow() |>
add_model(qda_mod) |>
add_recipe(dat_rec)
qda_fit <- qda_wf |> fit(data = dat_train) #entrainement du modèle en fonction du workflow et par rapport à la data d'entrainement
qda_preds <- predict(qda_fit, new_data = data_test, type = "prob") #predit la data_test grâce aux données d'entrainements
qda_result <- bind_cols(data_test, qda_preds)
qda_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, qda_result$.pred_no) %>% auc()
#| label: importation des packages
set.seed(1)
library(wooldridge)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
library(doParallel)
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
n <- nrow(ID_v6) #taille de la data
N <- round(1/10*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
ID_v7 <- ID_v6 %>% slice(train)
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
# dat_rec <- dat_train |> recipe(vhappy~educ+kids+tvhours) #TROP LOOOOOOOOOOOONG
dat_rec <- dat_train  %>% recipe(vhappy~.) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_zv(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = 0.9)
#| label: Définition du modèle SVM linéaire
svm_linear_mod <- svm_linear() |>
set_mode("classification") |>
set_engine("kernlab")
svm_linear_wf <- workflow() |>
add_model(svm_linear_mod |> set_args(cost = tune())) |>
add_recipe(dat_rec)
dat_folds <- vfold_cv(dat_train, v = 5, strata = vhappy)
# nombre de coeurs
n_core <- parallel::detectCores(logical = TRUE)
#
registerDoParallel(cores = n_core -1)
# on laisse un coeur pour le reste
# réalisation des commandes nécéssitant la parallélisation
svm_lin_grid <- grid_regular(cost(), levels = 5)
tune_res_svm_lin <- tune_grid(svm_linear_wf,
resamples = dat_folds,
grid = svm_lin_grid,
metrics = metric_set(roc_auc))
stopImplicitCluster()# ferme le cluster
autoplot(tune_res_svm_lin)
svm_lin_final_wf <- svm_linear_wf |> finalize_workflow((tune_res_svm_lin |> select_best(metric = "roc_auc")))
collect <- function(x){
last_fit(x,split = split_dat) |> # "je veux travailler sur tel truc en sachant le partage de donnée"
collect_predictions()
}
svm_lin_result <- svm_lin_final_wf |> collect()
svm_lin_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, svm_lin_result$.pred_no) %>% auc()
#| label: importation des packages
set.seed(123)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
n <- nrow(ID_v6) #taille de la data
N <- round(1/5*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
ID_v7 <- ID_v6 %>% slice(train)
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
#| label: Définition des 5 modèles
svm_rad_mod <- svm_rbf() |>
set_mode("classification") |>
set_engine("kernlab")
# dat_rec <- dat_train |> recipe(vhappy~educ+kids+tvhours) #TROP LOOOOOOOOOOOONG
dat_rec <- dat_train  %>% recipe(vhappy~.) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_zv(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = 0.9)
svm_rad_wf <- workflow() |>
add_model(svm_rad_mod |> set_args(margin = tune(),rbf_sigma = tune())) |>
add_recipe(dat_rec)
dat_folds <- vfold_cv(dat_train, v = 5, strata = vhappy) # 5 couches qui respectent tjrs le nombre gens très heureux
# nombre de coeurs
n_core <- parallel::detectCores(logical = TRUE)
registerDoParallel(cores = n_core -1)
# on laisse un coeur pour le reste
# réalisation des commandes nécéssitant la parallélisation
svm_rad_grid <- svm_rad_wf |> extract_parameter_set_dials() |> grid_regular(levels = 5)
tune_res_svm_rad <- tune_grid(svm_rad_wf,
resamples = dat_folds,
grid = svm_rad_grid)
stopImplicitCluster()# ferme le cluster
autoplot(tune_res_svm_rad)
svm_rad_final_wf <- svm_rad_wf |> finalize_workflow((tune_res_svm_rad |> select_best(metric = "roc_auc")))
collect <- function(x){
last_fit(x,split = split_dat) |> # "je veux travailler sur tel truc en sachant le partage de donnée"
collect_predictions()
}
svm_rad_result <- svm_rad_final_wf |> collect()
svm_rad_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, svm_rad_result$.pred_no) %>% auc()
#| label: importation des packages
set.seed(1)
library(wooldridge)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
library(doParallel)
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
#n <- nrow(ID_v6) #taille de la data
#N <- round(1*n) # Taille de l'échantillon d'entrainement
#train <- sample(1:n, size = N) # découpage de la data train
#ID_v7 <- ID_v6 %>% slice(train)
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
# dat_rec <- dat_train |> recipe(vhappy~educ+kids+tvhours) #TROP LOOOOOOOOOOOONG
dat_rec <- dat_train  %>% recipe(vhappy~.) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_zv(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = 0.9)
#| label: Définition du modèle SVM linéaire
svm_linear_mod <- svm_linear() |>
set_mode("classification") |>
set_engine("kernlab")
svm_linear_wf <- workflow() |>
add_model(svm_linear_mod |> set_args(cost = tune())) |>
add_recipe(dat_rec)
dat_folds <- vfold_cv(dat_train, v = 5, strata = vhappy)
# nombre de coeurs
n_core <- parallel::detectCores(logical = TRUE)
#
registerDoParallel(cores = n_core -1)
# on laisse un coeur pour le reste
# réalisation des commandes nécéssitant la parallélisation
svm_lin_grid <- grid_regular(cost(), levels = 5)
tune_res_svm_lin <- tune_grid(svm_linear_wf,
resamples = dat_folds,
grid = svm_lin_grid,
metrics = metric_set(roc_auc))
stopImplicitCluster()# ferme le cluster
autoplot(tune_res_svm_lin)
svm_lin_final_wf <- svm_linear_wf |> finalize_workflow((tune_res_svm_lin |> select_best(metric = "roc_auc")))
collect <- function(x){
last_fit(x,split = split_dat) |> # "je veux travailler sur tel truc en sachant le partage de donnée"
collect_predictions()
}
svm_lin_result <- svm_lin_final_wf |> collect()
svm_lin_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, svm_lin_result$.pred_no) %>% auc()
#| label: importation des packages
set.seed(123)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
#n <- nrow(ID_v6) #taille de la data
#N <- round(1/5*n) # Taille de l'échantillon d'entrainement
#train <- sample(1:n, size = N) # découpage de la data train
#ID_v7 <- ID_v6 %>% slice(train)
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
#| label: Définition des 5 modèles
svm_rad_mod <- svm_rbf() |>
set_mode("classification") |>
set_engine("kernlab")
# dat_rec <- dat_train |> recipe(vhappy~educ+kids+tvhours) #TROP LOOOOOOOOOOOONG
dat_rec <- dat_train  %>% recipe(vhappy~.) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_zv(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = 0.9)
svm_rad_wf <- workflow() |>
add_model(svm_rad_mod |> set_args(margin = tune(),rbf_sigma = tune())) |>
add_recipe(dat_rec)
dat_folds <- vfold_cv(dat_train, v = 5, strata = vhappy) # 5 couches qui respectent tjrs le nombre gens très heureux
# nombre de coeurs
n_core <- parallel::detectCores(logical = TRUE)
registerDoParallel(cores = n_core -1)
# on laisse un coeur pour le reste
# réalisation des commandes nécéssitant la parallélisation
svm_rad_grid <- svm_rad_wf |> extract_parameter_set_dials() |> grid_regular(levels = 5)
tune_res_svm_rad <- tune_grid(svm_rad_wf,
resamples = dat_folds,
grid = svm_rad_grid)
stopImplicitCluster()# ferme le cluster
autoplot(tune_res_svm_rad)
svm_rad_final_wf <- svm_rad_wf |> finalize_workflow((tune_res_svm_rad |> select_best(metric = "roc_auc")))
collect <- function(x){
last_fit(x,split = split_dat) |> # "je veux travailler sur tel truc en sachant le partage de donnée"
collect_predictions()
}
svm_rad_result <- svm_rad_final_wf |> collect()
svm_rad_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, svm_rad_result$.pred_no) %>% auc()
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
data <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
data <- data %>% mutate(across(everything(), ~replace_na(.x, "NA")))
data <- data %>% mutate(across(where(is.character), as.factor))
control.max <- rpart.control(cp = 0, max.depth = 0, minbucket = 1, minsplit = 1)
#| label: importation des packages
set.seed(1)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(MASS) #Modélisation LDA/QDA
library(naniar)
library(recipes)
library(dplyr)
library(rsample)
library(rpart)
library(rpart.plot)
control.max <- rpart.control(cp = 0, max.depth = 0, minbucket = 1, minsplit = 1)
tree <- rpart(vhappy~.,data = data, control = control.max,
parms = list(split = "information"))
prp(tree, type = 0, extra = 0, split.box.col = "lightblue", cex = 0.6)
control.max <- rpart.control(cp = 0, max.depth = 5, minbucket = 1, minsplit = 1)
tree <- rpart(vhappy~.,data = data, control = control.max,
parms = list(split = "information"))
prp(tree, type = 0, extra = 0, split.box.col = "lightblue", cex = 0.6)
set.seed(1)
data_split <- data |> initial_split(prop = 2/3)
test_data <- data_split |> testing()
train_data <- data_split |> training()
control.max <- rpart.control(cp = 0, max.depth = 5, minbucket = 1, minsplit = 1)
tree <- rpart(vhappy~.,data = train_data, control = control.max,
parms = list(split = "information"))
pred.tree <- predict(tree, newdata = train_data, type = "class")
table(pred.tree, train_data[,5])
table(pred.tree, train_data)
pred.tree <- predict(tree, newdata = train_data, type = "class")
table(pred.tree, train_data)
View(tree)
View(train_data)
#| label: importation des packages
set.seed(1)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(MASS) #Modélisation LDA/QDA
library(naniar)
library(recipes)
library(dplyr)
library(rsample)
library(rpart)
library(rpart.plot)
data <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
data <- data %>% mutate(across(everything(), ~replace_na(.x, "NA")))
data <- data %>% mutate(across(where(is.character), as.factor))
set.seed(1)
data_split <- data |> initial_split(prop = 2/3)
test_data <- data_split |> testing()
train_data <- data_split |> training()
control.max <- rpart.control(cp = 0, max.depth = 5, minbucket = 1, minsplit = 1)
tree <- rpart(vhappy~.,data = train_data, control = control.max,
parms = list(split = "information"))
prp(tree, type = 0, extra = 0, split.box.col = "lightblue", cex = 0.6)
#| label: importation des packages
set.seed(1)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(MASS) #Modélisation LDA/QDA
library(naniar)
library(recipes)
library(dplyr)
library(rsample)
library(rpart)
library(rpart.plot)
data <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
na = c("", "NA"))
data <- data %>% mutate(across(everything(), ~replace_na(.x, "NA")))
data <- data %>% mutate(across(where(is.character), as.factor))
set.seed(1)
data_split <- data |> initial_split(prop = 2/3)
test_data <- data_split |> testing()
train_data <- data_split |> training()
control.max <- rpart.control(cp = 0, max.depth = 0, minbucket = 1, minsplit = 1)
tree <- rpart(vhappy~.,data = train_data, control = control.max,
parms = list(split = "information"))
pred.tree <- predict(tree, newdata = train_data, type = "class")
table(pred.tree, train_data)
View(train_data)
table(pred.tree, train_data)
table(pred.tree, t(train_data))
ttrain_data <- train_data %>% t()
View(ttrain_data)
summary(pred.tree)
View(train_data)
pred.tree <- predict(tree, newdata = train_data, type = "class")
table(pred.tree, train_data[,10])
View(train_data)
table( train_data[,10])
table(pred.tree)
table(pred.tree,train_data[,10])
cbind(pred.tree,train_data[,10])
table(summary(pred.tree),summary(train_data[,10]))
pred.tree <- predict(tree, newdata = test_data, type = "class")
table(pred.tree, test_data$vhappy) # Species est la 5 eme colonne de test_data
mean(pred.tree != test_data$vhappy) # La 5eme collone de test_data est Species
# Graphique de cout de complexité
plotcp(tree)
tree$cptable
# cp_choisie à définir
cp_choisie <- sqrt(0.0035958288*0.0022773583) # moyenne géométrique des valeurs de 2 et 3 feuilles
treebis <- prune(tree, cp = cp_choisie) # en élaguant un arbre existant
control.prune <- rpart.control(cp = cp_choisie, max.depth = 0,
minbucket = 1, minsplit = 1)
treeter <- rpart(vhappy~. , data = train_data, control = control.prune,
parms = list(split = "information"))
prp(treebis, type = 1, extra = 1, split.box.col = "lightblue", cex = 0.6) # extra permet la création de feuille homogène.
plot(income~attend, data = train_data, col = train_data$vhappy)
abline(h=4.8)
plot(income~attend, data = train_data, col = train_data$vhappy)
mean(predict(treebis, newdata = train_data, type = "class") != train_data$vhappy)
pred.tree <- predict(treebis, newdata = test_data, type = "class")
table(pred.tree, test_data[,10])
pred.tree <- predict(treebis, newdata = test_data, type = "class")
table(pred.tree, test_data$vhappy)
mean(pred.tree != test_data$vhappy)
split_data <- initial_split(data, prop = 0.75, strata = data) # STRATA permet de garder la meme proportion de chacune des modalites de la variable spécifié. Dans le découpage on ne déséquilibre pas la modalité de la variable à predire. Surtout important quand il y des données desequilibré
split_data <- initial_split(data, prop = 0.75, strata = vhappy) # STRATA permet de garder la meme proportion de chacune des modalites de la variable spécifié. Dans le découpage on ne déséquilibre pas la modalité de la variable à predire. Surtout important quand il y des données desequilibré
data_train <- training(split_data)
data_test <- testing(split_data)
rec <- recipe(vhappy ~ ., data = data_train)
tree_spec <- decision_tree() |>
set_engine("rpart") |>
set_mode("classification")
library(doParallel)
n_core <- detectCores(logical = TRUE)
registerDoParallel(cores = n_core - 1)
# workflow avec paramètres à choisir
tune_tree_wf <- workflow() |>
add_model(tree_spec |>
set_args(cost_complexity = tune()) # les arguments sont soit une valeurs soit etre optimisé
) |>
add_recipe(rec)
# création des échantillons de validation croisée
data_cv <- vfold_cv(data_train) # on peut recommencer la VC pour solidifier le test
# valeurs à tester
cost_complexity_grid <- grid_regular(cost_complexity(range = c(-5,-0.1)), # Pas du vrai CART, on spécifie les valeurs à tester. Elles vont de 10^-5 à l'extremité proche du nombre entier
levels = 15)
#on a créée une grille de 15 niveau de cout de complexité
tree_tune_res <- tune_grid(
tune_tree_wf, # le workflow
resamples = data_cv, # les échantillons de validation croisée
grid = cost_complexity_grid, # la grille des valeurs à tester
metrics = metric_set(accuracy) # la métrique pour choisir la meilleur valeur
)
autoplot(tree_tune_res)
stopImplicitCluster()
tree_tune_res |> show_best(metric = "accuracy")
best_cost_complexity <- select_best(tree_tune_res, metric = "accuracy")
final_tune_tree_wf <- tune_tree_wf |>
finalize_workflow(best_cost_complexity)
tree_fit <- final_tune_tree_wf |> last_fit(split_data)
tree_fit |> collect_metrics()
tree_fit |> collect_predictions()
tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_No) |> autoplot()
tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_no) |> autoplot()
roc_curve(vhappy, .pred_no) %>% auc()
roc(data_test$vhappy, tree_fit$.pred_no) %>% auc()
View(tree_fit)
tf <- tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_no)
View(tf)
tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_no) |> autoplot()
tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_no) |> autoplot()
tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_no) %>% auc()
library(yardstick)
# Vérifie que 'vhappy' est bien un facteur
tree_preds <- tree_preds |> mutate(vhappy = as.factor(vhappy))
library(yardstick)
tree_preds <- tree_fit |> collect_predictions()
# Vérifie que 'vhappy' est bien un facteur
tree_preds <- tree_preds |> mutate(vhappy = as.factor(vhappy))
# Courbe ROC et AUC
roc_curve(tree_preds, truth = vhappy, .pred_no) |>
autoplot()
roc_auc(tree_preds, truth = vhappy, .pred_no)
# Courbe ROC et AUC
roc_curve(tree_fit, truth = vhappy, .pred_no) |>
autoplot()
roc_curve(tree_fit, truth = vhappy, .pred_no) |>
autoplot()
tree_fit |> collect_predictions() |> roc_curve(vhappy, .pred_no) |> autoplot()
tree_preds <- tree_fit |> collect_predictions()
# Vérifie que 'vhappy' est bien un facteur
tree_preds <- tree_preds |> mutate(vhappy = as.factor(vhappy))
# Courbe ROC et AUC
roc_curve(tree_fit, truth = vhappy, .pred_no) |>
autoplot()
tree_preds <- tree_fit |> collect_predictions()
# Vérifie que 'vhappy' est bien un facteur
tree_preds <- tree_preds |> mutate(vhappy = as.factor(vhappy))
# Courbe ROC et AUC
roc_curve(tree_preds, truth = vhappy, .pred_no) |>
autoplot()
roc_auc(tree_preds, truth = vhappy, .pred_no)
tree_fit |>
extract_fit_engine() |>
rpart.plot::prp(type = 0, extra = 1, split.box.col = "lightblue",
roundint = FALSE)
final_model <-  final_tune_tree_wf |>
fit(data) # données complètes
save(final_model, file = "model_v1.RData")
a_predire <- data.frame(
year = c(1994,1998,2004),
workstat = c("retired","keeping house","other"),
prestige = c(12,54,32),
DivWid = c("yes","no","yes"),
educ = c(7,16,11),
kids = c(0,3,5),
income = c("$10000 - 14999","$25000 or more","lt $1000"),
region = c("new england","w. sou. central","
pacific"),
attend = c(6,30,104),
owngun = c("iap","yes","NA"),
tvhours = c(1,4,9),
vhappy = c("yes","no","yes"),
mothfath16 = c(FALSE,TRUE,TRUE),
black = c(TRUE,FALSE,TRUE),
female = c(TRUE,TRUE,FALSE),
unem10 = c("no","yes","NA")
)
# chargement du modèle
load("model_v1.RData")
# cette commande créera un objet du même nom qu'au départ
predict(final_model, new_data = a_predire) # par défaut type = "class"
predict(final_model, new_data = a_predire, type = "prob")
table(data$workstat)

---
title: "LDA / QDA Very Happiness"
format: html
---

# Packages

```{r}
#| label: importation des packages
set.seed(1)
library(wooldridge)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
library(kknn)
library(doParallel)
```
# Importation des données

```{r}
ID_v6 <- read_csv("D:/Université de Tours/Master MEcEn/S8/8-2 Classification supervisée/Projet HAPPINESS/R_ happiness_files/vhappy/vhappy.csv",
                  na = c("", "NA"))
ID_v6 <- ID_v6 %>% mutate(across(everything(), ~replace_na(.x, "NA")))
ID_v6 <- ID_v6 %>% mutate(across(where(is.character), as.factor))
```

# 10% de la data
```{r}
#n <- nrow(ID_v6) #taille de la data
#N <- round(1*n) # Taille de l'échantillon d'entrainement
#train <- sample(1:n, size = N) # découpage de la data train
#ID_v7 <- ID_v6 %>% slice(train)
```

# Echantillonage 

```{r}
set.seed(1)
split_dat <- initial_split(ID_v7, prop = 0.75, strata = vhappy) # classique 3 quart / 1 quart
dat_train <- training(split_dat)
data_test <- testing(split_dat)
```

## Construction des différents modèles

### recette à appliquer 

```{r}
# dat_rec <- dat_train |> recipe(vhappy~educ+kids+tvhours) #TROP LOOOOOOOOOOOONG

dat_rec <- dat_train  %>% recipe(vhappy~.) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_other(all_nominal_predictors(), threshold = 0.05) %>% 
  step_zv(all_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)
```

```{r}
#| label: Définition du modèle SVM linéaire 

svm_linear_mod <- svm_linear() |> 
  set_mode("classification") |> 
  set_engine("kernlab")
```

```{r}
svm_linear_wf <- workflow() |> 
  add_model(svm_linear_mod |> set_args(cost = tune())) |> 
  add_recipe(dat_rec)
```

### Echantillonage pour l'optimisation des paramètres

Pour l'optimisation des hyper paramètres --> ADOCK
```{r}
dat_folds <- vfold_cv(dat_train, v = 5, strata = vhappy)
```


Grille pour l’optimisation du paramètre svm linéaire.

```{r}
# nombre de coeurs 
n_core <- parallel::detectCores(logical = TRUE)
#
registerDoParallel(cores = n_core -1) 
# on laisse un coeur pour le reste
# réalisation des commandes nécéssitant la parallélisation

svm_lin_grid <- grid_regular(cost(), levels = 5)
tune_res_svm_lin <- tune_grid(svm_linear_wf,
                              resamples = dat_folds,
                              grid = svm_lin_grid,
                              metrics = metric_set(roc_auc))

stopImplicitCluster()# ferme le cluster
```

```{r}
autoplot(tune_res_svm_lin)
```

On selectionne la meilleur valeur de margin :

```{r}
svm_lin_final_wf <- svm_linear_wf |> finalize_workflow((tune_res_svm_lin |> select_best(metric = "roc_auc")))
```


Les paramètres sont optimisés on peut maintenant faire travailler les modèles sur les données d'apprentissage et comparer les qualités des modèles sur les données test.

```{r}
collect <- function(x){
  last_fit(x,split = split_dat) |> # "je veux travailler sur tel truc en sachant le partage de donnée"
    collect_predictions()
}
```

```{r}
svm_lin_result <- svm_lin_final_wf |> collect()
```

On va construitre un tableau qui continent les résultats pour chaque individus de la base de donnée testing, en fonctioon de la méthode utilisé
Pour chasue individu : - résultat, -méthode utilisé


Comparons les modèles :

```{r}
svm_lin_result |> roc_curve(vhappy, .pred_no) |> autoplot()
roc(data_test$vhappy, svm_lin_result$.pred_no) %>% auc()
```
Si proche de la bissectrice, le hasard est aussi bon que le modèle.




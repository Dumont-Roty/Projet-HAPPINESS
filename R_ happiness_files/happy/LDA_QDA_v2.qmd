---
title: "LDA / QDA_happiness"
format: html
embed-resources: true
---

# Packages

```{r}
#| label: importation des packages
set.seed(1)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kableExtra)
library(recipes)
library(MASS) #Modélisation LDA/QDA
library(pROC)
```

# Importation des données v1


```{r}
#| label: Package de données

library(wooldridge)
data("happiness") #base de données


ID_v1 <- happiness[, c(1:19,21:22,33)] %>%
  mutate(
    divorce = case_when(
      divorce == "yes" ~ "yes",
      divorce == "no" ~ "no",
      divorce == "iap" ~ "iap",
      is.na(divorce) ~ "NA"
    ),
    widowed = case_when(
      widowed == "yes" ~ "yes",
      widowed == "iap" ~ "iap",
      widowed == "no" ~ "no",
      is.na(widowed) ~ "NA"
    ),
    owngun = case_when(
      owngun == "yes" ~ "yes",
      owngun == "iap" ~ "iap",
      owngun == "no" ~ "no",
      is.na(owngun) ~ "NA"
    ),
    unem10 = case_when(
      unem10 == "1" ~ "yes",
      unem10 == "0" ~ "no",
      is.na(unem10) ~ "NA"
    ),
    attend = case_when(
      attend == "never" ~ "0",
      attend == "lt once a year" ~ "0.5",
      attend == "once a year" ~ "1",
      attend == "sevrl times a yr" ~ "6",
      attend == "once a month" ~ "12",
      attend == "2-3x a month" ~ "30",
      attend == "nrly every week" ~ "42",
      attend == "every week" ~ "52",
      attend == "more thn once wk" ~ "104",
      is.na(attend) ~ "NA"
    ),
  ) %>%
  mutate(
    year = as.factor(year),
    workstat = as.factor(workstat),
    prestige = as.numeric(prestige),
    divorce = as.factor(divorce),
    widowed = as.factor(widowed),
    educ = as.numeric(educ),
    reg16 = as.factor(reg16),
    babies = as.numeric(babies),
    preteen = as.numeric(preteen),
    teens = as.numeric(teens),
    income = as.factor(income),
    region = as.factor(region),
    attend = as.numeric(attend),
    happy = as.factor(happy),
    owngun = as.factor(owngun),
    tvhours = as.numeric(tvhours),
    mothfath16 = as.logical(mothfath16),
    black = as.logical(black),
    female = as.logical(female),
    blackfemale = as.logical(blackfemale),
    unem10 = as.factor(unem10)
  )
```

### Enfants / pré-ados / ados v2

```{r}
ID_b <- ID_v1 %>% filter(is.na(babies))
rbind(NA_filtré = ID_b[,c(8:10)] %>% is.na() %>% colSums(),NA_N_filtré = ID_v1[,c(8:10)] %>% is.na() %>% colSums())

ID_v2 <- ID_v1 %>% filter(babies != "NA")
```

### imputation de données manquantes de la variable *attend*

```{r}
n <- nrow(ID_v2) #taille de la data
N <- round(2/3*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
dataTrain_att <- ID_v2 %>% slice(train)
dataTest_att <- ID_v2 %>% slice(-train)

rec_att <- recipe(~., data = ID_v2) %>% 
  step_impute_knn(attend, neighbors = 5)

rec_att_prep <- prep(rec_att, training = dataTrain_att)
ID_v3 <- bake(rec_att_prep, new_data = ID_v2)
```

### Imputation des données manquantes de income

```{r}
n <- nrow(ID_v3) #taille de la data
N <- round(2/3*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
dataTrain_i <- ID_v3 %>% slice(train)
dataTest_i <- ID_v3 %>% slice(-train)

# induce some missing data at random
set.seed(1)

rec_i <- recipe(~ .,data = ID_v3) %>% 
  step_impute_knn(income, neighbors = 5)

rec_i_prep <- prep(rec_i, dataTrain_i)
ID_v4 <- bake(rec_i_prep, new_data = ID_v3)
```

### imputation de données manquantes de la variable *prestige*

```{r}
n <- nrow(ID_v4) #taille de la data
N <- round(2/3*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
dataTrain_p <- ID_v4 %>% slice(train)
dataTest_p <- ID_v4 %>% slice(-train)

rec_p <- recipe(~., data = ID_v4) %>% 
  step_impute_knn(prestige, neighbors = 5)

rec_p_prep <- prep(rec_p, training = dataTrain_p)
ID_v5 <- bake(rec_p_prep, new_data = ID_v4)
```

### Nettoyage des valeurs aberrantes

```{r}
#ID_v5 <- ID_v5[ID_v5$tvhours != 24, ] si aberrant car 4 ont mis + de 24h
# Est ce qu'on retire tout ceux au dessus de 18h ?
# ID_v5 <- ID_v5[ID_v5$babies != 6, ]
# 1 personne à 6 bébé l'autre max est à 4
```

```{r}
table(ID_v5$tvhours)
```


### imputation de données manquantes de la variable *tvhours*

```{r}
n <- nrow(ID_v5) #taille de la data
N <- round(2/3*n) # Taille de l'échantillon d'entrainement
train <- sample(1:n, size = N) # découpage de la data train
dataTrain_tv <- ID_v5 %>% slice(train)
dataTest_tv <- ID_v5 %>% slice(-train)

rec_tv <- recipe(~., data = ID_v5) %>% 
  step_impute_knn(tvhours, neighbors = 5)

rec_tv_prep <- prep(rec_tv, training = dataTrain_tv)
ID_v6 <- bake(rec_tv_prep, new_data = ID_v5)
```

# Suppression des modalités vide
```{r}
ID_v6$happy <- droplevels(ID_v6$happy)
# J'ai supprimé les individus à qui il restait une MDA
ID_v6 <- na.omit(ID_v6)
```

# LDA / QDA (sans tidymodels)

## Echantillonnage

```{r}
n <- nrow(ID_v6) #taille de la data
N <- round(2/3*n) # Taille de l'échantillon d'entrainement
train <- sample(size = N, x = 1:n) # découpage de la data train
dataTrain <- ID_v6 %>% slice(train)
dataTest <- ID_v6 %>% slice(-train)
```

### Analyse linéaire discriminante

Doit prendre en compte des variables numéraire

#### Modélisation

```{r}
lda.fit <- lda(happy ~ prestige + educ + babies + preteen + teens + attend + tvhours, data = dataTrain) # Création du modèle d'entrainement
```

On observe les sorties des commandes suivantes :

```{r}
summary(lda.fit)
str(lda.fit)
```

#### Affectation

```{r}
# Problème ici
lda.predict <- lda.fit |> predict(newdata = dataTest)
```

#### Qualité du modèle

```{r}
#| label: Matrice de confusion
Réalité <- dataTest$happy

 Prédictions<-lda.predict$class
 
 tab0<-table(dataTest$happy,lda.predict$class) %>% addmargins()
 
 tab<-tab0 %>%matrix(nrow=nrow(tab0)) %>%
 as_tibble()%>%
 add_column(Réalité=c("Very Happy","Pretty Happy","Not to Happy","Total"),.before=1)%>%
 rename("Very Happy"=V1,"Pretty Happy"=V2,"Not to Happy"=V3, Total = V4)
 
 tab %>%
  kable() %>%
  add_header_above(c("", "Prédiction" = 3, "")) %>%
  column_spec(5, bold = TRUE, background = "#F4F6F6", width = "2cm") %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(4, bold = TRUE, background = "#F4F6F6") %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(position = "center", full_width = FALSE, bootstrap_options = "bordered", latex_options = "hold_position")
```

### QDA (Analyse quadratique discriminante)

Doit prendre en compte des variables numéraire

#### Modélisation

```{r}
qda.fit <- qda(happy ~ prestige + educ + babies + preteen + teens + attend + tvhours, data = dataTrain) # Création du modèle d'entrainement
```

#### Affectation

```{r}
# Problème ici
qda.predict <- qda.fit |> predict(newdata = dataTest)
```

#### Qualité du modèle

```{r}
#| label: Matrice de confusion
Réalité <- dataTest$happy

 Prédictions<-qda.predict$class
 
 tab0<-table(dataTest$happy,qda.predict$class) %>% addmargins()
 
 tab<-tab0 %>%matrix(nrow=nrow(tab0)) %>%
 as_tibble()%>%
 add_column(Réalité=c("Very Happy","Pretty Happy","Not too Happy","Total"),.before=1)%>%
 rename("Very Happy"=V1,"Pretty Happy"=V2,"Not too Happy"=V3, Total = V4)
 
 tab %>%
  kable() %>%
  add_header_above(c("", "Prédiction" = 3, "")) %>%
  column_spec(5, bold = TRUE, background = "#F4F6F6", width = "2cm") %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(4, bold = TRUE, background = "#F4F6F6") %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(position = "center", full_width = FALSE, bootstrap_options = "bordered", latex_options = "hold_position")
```


```{r}
#| label: Matrice de confusion (%)
Réalité <- dataTest$happy

 Prédictions <-qda.predict$class
 
 tab0<-table(dataTest$happy,qda.predict$class) %>% 
 proportions(margin = 1) |> #permet de mettre en proportion 
 addmargins(margin = 2) |> 
 round(2) #on arrondfis pour plus de visibilité
 tab <- tab0 |> matrix(nrow=nrow(tab0)) |> 
 as_tibble() |> 
 add_column(Réalité=c("Very Happy","Pretty happy","Not too Happy"),.before=1)%>%
 rename("Very Happy"=V1,"Pretty happy"=V2,"Not too Happy"=V3, Total = V4)
 tab |> kable() |> 
 add_header_above(c("","Prédiction"=3,"")) |> 
 column_spec(c(5),bold=T,background="#F4F6F6",width="2cm")  |> 
 column_spec(1,bold=T) |> 
 row_spec(c(0),bold=T) |> 
 kable_styling(full_width=FALSE,
 bootstrap_options="bordered",
 latex_options="hold_position")
```

# LDA / QDA (todymodels)

## Modèle linéaire

### Création du modèle

```{r}
lda_spec <- discrim_linear() |> 
  set_mode("classification") |> 
  set_engine("MASS")
lda_fit <- lda_spec |> 
  fit(happy~prestige + educ + babies + preteen + teens + attend + tvhours,data = dataTrain)
```

### Obtention de la matrice de confusion sur les données test

```{r}
predict(lda_fit, new_data = dataTrain, type = "prob")
```

```{r}
tab <- augment(lda_fit,new_data = dataTest) |> conf_mat(truth=happy,estimat=.pred_class)


tab[[1]]%>%
 matrix(nrow=nrow(.))%>%
 t()%>%
 addmargins()%>%
 as_tibble()%>%
 add_column(Réalité=c("Very Happy","Pretty Happy","Not to Happy","Total"),.before=1)%>%rename("Very Happy"=V1,"Pretty Happy"=V2,"Not to Happy"=V3,Total=Sum)%>%
 kable()%>%
 add_header_above(c("","Prédiction"=3,""))%>%
 column_spec(c(5),bold=T,background="#F4F6F6",width="2cm") %>%
 column_spec(1,bold=T)%>%
 row_spec(c(4),bold=T,background="#F4F6F6") %>%
 row_spec(c(0),bold=T)%>%
 kable_styling(position="center",
 full_width=FALSE,
 bootstrap_options = "bordered",
 latex_options = "hold_position")
```

## QDA

### Création du modèle

```{r}
qda_spec <- discrim_quad() |> 
  set_mode("classification") |> 
  set_engine("MASS")
qda_fit <- qda_spec |> 
  fit(happy ~ prestige + educ + babies + preteen + teens + attend + tvhours, data = dataTrain)
```

### Obtention matrice de confusion sur les données test

```{r}
predict(qda_fit, new_data = dataTest, type = "prob")
```

```{r}
tab <- augment(qda_fit,new_data = dataTest) |> conf_mat(truth=happy,estimat=.pred_class)

tab[[1]]%>%
 matrix(nrow=nrow(.))%>%
 t()%>%
 addmargins()%>%
 as_tibble()%>%
 add_column(Réalité=c("Very Happy","Pretty Happy","Not to Happy","Total"),.before=1)%>%rename("Very Happy"=V1,"Pretty Happy"=V2,"Not to Happy"=V3,Total=Sum)%>%
 kable()%>%
 add_header_above(c("","Prédiction"=3,""))%>%
 column_spec(c(5),bold=T,background="#F4F6F6",width="2cm") %>%
 column_spec(1,bold=T)%>%
 row_spec(c(4),bold=T,background="#F4F6F6") %>%
 row_spec(c(0),bold=T)%>%
 kable_styling(position="center",
 full_width=FALSE,
 bootstrap_options = "bordered",
 latex_options = "hold_position")
```

On voit que dans les prédiction du modèle quadratique: - la modalité "Very Happy" est prédite 126 fois mais fait 77 erreurs. - la modalité "Pretty happy" est prédite 5408 fois dont `r 1720+652` erreurs. - la modalité "Not to happy" est prédite 41 fois mais fait 33 erreurs

Il y a au total 3093 bonne prédiction et 2482 erreurs de prédiction.

On a, grâce au modèle de l'analyse quadratique discriminante, un taux de bonne prédiction de `r round(3093/5575*100,2)`%

### Matrice des confusions en proportion

```{r}
#| label: matrice des confusions (proportion)
pourcent <- function(x) { 100 * x }


# Transformation et mise en forme du tableau
tab[[1]] %>%
  proportions(margin = 2) %>%
  pourcent() %>% 
  matrix(nrow = nrow(.)) %>%
  t() %>%
  addmargins(margin = 2) %>%
  as_tibble() %>%
  add_column(Réalité = c("Very Happy", "Pretty Happy", "Not to Happy"), .before = 1) %>%
  rename("Very Happy" = V1, "Pretty Happy" = V2, "Not to Happy" = V3, "Total" = Sum) %>%
  kable(digits = 1) %>%  # Arrondi à 1 décimale
  add_header_above(c("", "Prédiction" = 3, "")) %>%
  column_spec(5, bold = TRUE, background = "#F4F6F6", width = "2cm") %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(
    position = "center",
    full_width = FALSE,
    bootstrap_options = "bordered",
    latex_options = "hold_position"
  )
```

```{r}
#| label: probabilité
qda.predict$posterior  |>  head()  |>  
  kable()  |>  
  kable_styling(full_width = FALSE,
                bootstrap_options = "bordered",
                latex_options = "hold_position")
```

### Abaissement du seuil

```{r}
#| label: seuil
class20 <- case_when(
  qda.predict$posterior[,2] > 0.6 ~ "Very Happy", # Seuil à retravailler ?
  qda.predict$posterior[,2] > 0.2 ~ "Pretty Happy",
  TRUE ~ "Not too Happy"
) |> as.factor()
```

```{r}
#| label: matrice des confusions 2

tab20 <- table(Réalité = dataTest$happy, Prédiction = class20)  |>  addmargins()

tab20 <- tab20  |>  matrix(nrow=nrow(tab20))  |> 
  as_tibble()  |> 
  add_column(Réalité=c("Very Happy", "Pretty Happy", "Not too Happy", "Total"),.before=1)  |> 
  rename("Very Happy"=V1, "Pretty Happy"=V2, "Not too Happy"=V3, "Total"=V4)

tab20  |>  kable()  |>  
  add_header_above(c(" ","Prédiction" = 3," "))  |> 
  column_spec(c(5), bold = T, background ="#F4F6F6" ,width="2cm")  |> 
  column_spec(1,bold=T)  |> 
  row_spec(c(4), bold = T, background ="#F4F6F6" )  |> 
  row_spec(c(0), bold = T)  |> 
  kable_styling(position="center",full_width = FALSE,
                bootstrap_options = "bordered",
                latex_options = "hold_position")

```

# Etude des erreurs en fonction du seuil
```{r}
# Création du vecteur seuil
Seuil <- seq(0, 1, by = 0.01) # On fait varier le seuil de 0 à 0.5 

# Initialisation des vecteurs d'erreurs
ErrorI <- numeric(length(Seuil))  # Erreur de première espèce 
ErrorII <- numeric(length(Seuil)) # Erreur de seconde espèce
Error <- numeric(length(Seuil)) # Erreur globale

# Variables de réalité
Realite <- dataTest$happy
V <- sum(Realite == "very happy")
NV <- sum(Realite == "pretty happy" | Realite == "not too happy")

# Boucle sur les seuils
for (i in seq_along(Seuil)) {
  c <- Seuil[i]

  # Classification en fonction du seuil
  classc <- ifelse(lda.predict$posterior[, 2] > c, "very happy",
                   ifelse(lda.predict$posterior[, 3] > c, "pretty happy", "not too happy"))
  
  # Calcul des erreurs
  Error[i] <- sum(classc != Realite) / length(Realite)  # Erreur globale
  
  ErrorI[i] <- sum((classc == "very happy") & (Realite == "pretty happy" | Realite == "not too happy")) / V # Fausse alarme
  ErrorII[i] <- sum((classc == "pretty happy" | classc == "not too happy") & (Realite == "very happy")) / NV # Mauvaise classification
}
# Commentaire : À seuil 0, on classe tout en "very happy", puis peu à peu on classe davantage dans "pretty happy" et "not too happy".
```


```{r}
plot(Seuil, Error, type = "l", ylim = c(0,5))
lines(Seuil, ErrorI, type = "l", lty = "dotted", col = "purple")
lines(Seuil, ErrorII, type = "l", lty = "dashed", col = "blue")
```

# Courbe ROC avec Tidymodels


```{r}
tabroc <- roc(dataTest$happy, lda.predict$posterior[,1])
# compare la réalité avec les résultats des proba a posteriori
ggroc(tabroc)
auc(tabroc)
```


